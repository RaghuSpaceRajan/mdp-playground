{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to analyse an MDP Playground experiment\n",
    "from mdp_playground.analysis import MDPP_Analysis\n",
    "# Set dir_name to the location where the CSV files from running an experiment were saved\n",
    "dir_name = '../mdp_files/'\n",
    "# Set exp_name to the name that was given to the experiment when running it\n",
    "exp_name = 'dqn_seq_del'\n",
    "# Set the following to True to save PDFs of plots that you generate below\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "save_folder = \"../mdp_files/plots/\"\n",
    "mdpp_analysis = MDPP_Analysis(save_folder)\n",
    "train_stats, eval_stats, train_curves, eval_curves, train_aucs, eval_aucs = mdpp_analysis.load_data(dir_name, exp_name, load_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D: Plots showing reward after 20k timesteps when varying a single meta-feature\n",
    "# Plots across 10 runs: Training: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(train_stats, save_fig)\n",
    "mdpp_analysis.plot_1d_dimensions(train_aucs, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots across 10 runs: Evaluation: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(eval_stats, save_fig, train=False)\n",
    "mdpp_analysis.plot_1d_dimensions(eval_aucs, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This and the next cell do the same as the previous 2 cells but plot episode mean lengths instead of episode reward\n",
    "mdpp_analysis.plot_1d_dimensions(train_stats, save_fig, metric_num=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpp_analysis.plot_1d_dimensions(eval_stats, save_fig, train=False, metric_num=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Training runs: with std dev across the runs\n",
    "# There seems to be a bug with matplotlib - x and y axes tick labels are not correctly set even though we pass them. Please feel free to look into the code and suggest a correction if you find it.\n",
    "mdpp_analysis.plot_2d_heatmap(train_stats, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Evaluation runs: with std dev across the runs\n",
    "mdpp_analysis.plot_2d_heatmap(eval_stats, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Training: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(train_curves, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Evaluation: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(eval_curves, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cave.cavefacade import CAVE\n",
    "from mdp_playground.analysis.mdpp_to_cave import MDPPToCave\n",
    "import os\n",
    "\n",
    "#The converted mdpp csvs will be stored in output_dir\n",
    "output_dir = \"../mdpp_to_cave\"\n",
    "mdpp_cave = MDPPToCave(output_dir)\n",
    "cave_input_file = mdpp_cave.to_bohb_results(dir_name, exp_name)\n",
    "\n",
    "cave_input_file = \"../mdpp_to_cave/dqn_seq_del\"\n",
    "\n",
    "# Similarly, as an example, cave will ouput it's results \n",
    "# to the same directory as cave's input files\n",
    "\n",
    "cave_results = os.path.join(cave_input_file, \"out\")\n",
    "cave = CAVE(folders = [cave_input_file],\n",
    "            output_dir = cave_results,\n",
    "            ta_exec_dir = [cave_input_file],\n",
    "            file_format = \"BOHB\",\n",
    "            show_jupyter=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave.compare_default_incumbent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave.performance_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave.cave_fanova()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave.local_parameter_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "frc = [-1] + mdpp_analysis.final_rows_for_a_config\n",
    "print(len(frc), frc, type(train_curves))\n",
    "j = 20\n",
    "for i in range(5):\n",
    "    plt.plot(train_curves[frc[j+i]+1:frc[j+i+1], -2])\n",
    "plt.ylim([200, 250])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data = train_stats\n",
    "metric_num = -2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mean_data_ = np.mean(stats_data[..., metric_num], axis=-1) # the slice sub-selects the metric written in position metric_num from the \"last axis of diff. metrics that were written\" and then the axis of #seeds becomes axis=-1 ( before slice it was -2).\n",
    "to_plot_ = np.squeeze(mean_data_)\n",
    "std_dev_ = np.std(stats_data[..., metric_num], axis=-1) #seed\n",
    "to_plot_std_ = np.squeeze(std_dev_)\n",
    "\n",
    "#fig_width = len(self.tick_labels[0])\n",
    "fig_width = 5\n",
    "# plt.figure()\n",
    "plt.figure(figsize=(fig_width, 1.5))\n",
    "\n",
    "print(to_plot_.shape)\n",
    "plt.bar([i for i in range(to_plot_.shape[0])], to_plot_, yerr=to_plot_std_)\n",
    "plt.ylim([-5, 0])\n",
    "plt.grid()\n",
    "# plt.bar(self.tick_labels[0], to_plot_[:, 0], yerr=to_plot_std_[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_stats.shape, eval_stats.shape, train_curves.shape, eval_curves.shape)\n",
    "print(train_stats[:,0:4,:,:])\n",
    "ts_copy = train_stats.copy()\n",
    "ts_copy[:,1:,:,:] = train_stats[:,:-1,:,:]\n",
    "ts_copy[:,0,:,:] = train_stats[:,4,:,:]\n",
    "\n",
    "tc_copy = train_curves.copy()\n",
    "tc_copy[14955:,:] = train_curves[:-14955,:]\n",
    "tc_copy[:14955,:] = train_curves[-14955:,:]\n",
    "# 14955\n",
    "# mdpp_analysis.tick_labels[0][1:5], mdpp_analysis.tick_labels[0][0] = mdpp_analysis.tick_labels[0][0:4], mdpp_analysis.tick_labels[0][4]\n",
    "# mdpp_analysis.dims_values[1][1:5], mdpp_analysis.dims_values[1][0] = mdpp_analysis.dims_values[1][0:4], mdpp_analysis.dims_values[1][4]\n",
    "print(ts_copy)\n",
    "print(dir(mdpp_analysis))\n",
    "print(mdpp_analysis.metric_names, mdpp_analysis.tick_labels)\n",
    "print(mdpp_analysis.dims_values[1], mdpp_analysis.config_names, mdpp_analysis.dims_varied)\n",
    "print(train_curves[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some more analysis (for tune HPs)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr as spm\n",
    "from scipy.stats import pearsonr as prs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir_name_config = 'experiments/'\n",
    "file_ = dir_name_config + exp_name\n",
    "\n",
    "config_file_path = os.path.abspath('/'.join(file_.split('/')[:-1]))\n",
    "# print(file_.split('/')[:-1])\n",
    "print(\"config_file_path:\", config_file_path)\n",
    "sys.path.insert(1, config_file_path) #hack\n",
    "import importlib\n",
    "config = importlib.import_module(file_.split('/')[-1], package=None)\n",
    "print(\"Number of seeds for environment:\", config.num_seeds)\n",
    "\n",
    "value_tuples = []\n",
    "for config_type, config_dict in config.var_configs.items():\n",
    "    for key in config_dict:\n",
    "        if 'seed' in key:\n",
    "            print(\"Found seed axis:\", key)\n",
    "            pass\n",
    "        else:\n",
    "            assert type(config.var_configs[config_type][key]) == list, \"var_config should be a dict of dicts with lists as the leaf values to allow each configuration option to take multiple possible values\"\n",
    "            value_tuples.append(config.var_configs[config_type][key])\n",
    "print(\"value_tuples\", value_tuples)\n",
    "\n",
    "import itertools\n",
    "cartesian_product_configs = list(itertools.product(*value_tuples))\n",
    "print(\"Total number of configs. to run:\", len(cartesian_product_configs))\n",
    "print(\"Varying dims in mdpp_analysis.axis_labels (will have dummy_seed in there as 1st dim and may not show actual last varying dim because mdpp_analysis assumes last varying dim is seed (and ignores last config_name) and here seed is always the 1st dim):\", mdpp_analysis.axis_labels)\n",
    "# import itertools\n",
    "# cartesian_product_configs = list(itertools.product(*config_vals))\n",
    "for i in range(len(train_stats.shape)):\n",
    "    if train_stats.shape[i] > 1:\n",
    "        dummy_seeds_axis = i\n",
    "        break\n",
    "print(\"dummy_seeds_axis, train_stats.shape:\", dummy_seeds_axis, len(train_stats.shape))\n",
    "\n",
    "# dummy_seeds_axis = -1\n",
    "\n",
    "def analysis(train_stats):\n",
    "    mean_data_ = np.mean(train_stats[..., -2], axis=dummy_seeds_axis)\n",
    "    std_data_ = np.std(train_stats[..., -2], axis=dummy_seeds_axis)\n",
    "    print(\"Mean shape (after slice), Sliced shape:\", mean_data_.shape, train_stats[..., -2].shape)\n",
    "    flattened_mean = np.ravel(mean_data_)\n",
    "    flattened_std = np.ravel(std_data_)\n",
    "    ranks = np.argsort(flattened_mean)[::-1]\n",
    "    print('sort of indices:\\n', ranks)\n",
    "    ranks_with_std = np.argsort(flattened_mean - flattened_std)[::-1]\n",
    "    print('sort of indices (with std taken into account):\\n', ranks_with_std)\n",
    "    sorted_vals = np.sort(flattened_mean)[::-1]\n",
    "    print('sort of values:\\n', sorted_vals)\n",
    "    sorted_vals_with_std = np.sort(flattened_mean - flattened_std)[::-1]\n",
    "    print('sort of values (with std taken into account):\\n', sorted_vals_with_std)\n",
    "    print(\"TOP 3 configs (with std taken into account):\")\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-1]]) \n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-2]])\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-3]])\n",
    "    print(\"\\nBOTTOM 3 configs (with std taken into account):\")\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[0]]) \n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[1]])\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[2]])\n",
    "    plt.figure(figsize=(30, 1.5))\n",
    "    plt.bar([i for i in range(len(flattened_mean))], flattened_mean, yerr=flattened_std)\n",
    "    plt.show()\n",
    "    return flattened_mean, flattened_mean - flattened_std\n",
    "\n",
    "sorted_vals_t, sorted_vals_with_std_t = analysis(train_stats)\n",
    "sorted_vals_e, sorted_vals_with_std_e = analysis(eval_stats)\n",
    "print(spm(sorted_vals_t, sorted_vals_e))\n",
    "print(spm(sorted_vals_t, sorted_vals_with_std_t))\n",
    "print(spm(sorted_vals_with_std_t, sorted_vals_with_std_e))\n",
    "print(spm(sorted_vals_e, sorted_vals_with_std_e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.var_configs)\n",
    "print(train_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpp_analysis.config_names, mdpp_analysis.config_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(train_stats, axis=2))\n",
    "print(np.mean(train_aucs, axis=2))\n",
    "from scipy import stats\n",
    "print(stats.spearmanr([4,3,2,1,0], [4,3,2,1,0]))\n",
    "print(np.mean(eval_stats, axis=2))\n",
    "print(np.mean(eval_aucs, axis=2))\n",
    "print(train_stats)\n",
    "print(eval_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
